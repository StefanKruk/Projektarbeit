%Diese Datei enthält eine beispielhafte Literaturdatenbank.
%
%Für LaTeX existieren verschiedene Literaturverwaltungssysteme, die diese Datei verarbeiten und daraus ein Literaturverzeichnis erstellen. Das Standardprogramm zur Literaturverwaltung in LaTeX ist das Programm BibTeX. Daneben ist noch das Programm biber in Kombination mit dem Paket biblatex verbreitet.
%
%Als Einstieg in die Literaturverwaltungssysteme sei auf die folgenden URLs verwiesen:
% https://de.wikipedia.org/wiki/BibTeX
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management
% http://tex.stackexchange.com/questions/25701/bibtex-vs-biber-and-biblatex-vs-natbib
%
% Die LaTeX-Vorlage ist derzeit für BibTeX vorkonfiguriert.


@book{Book,
    author    = {P.P. Autor},
    title     = {Titel des Buchs},
    publisher = {Verlag},
    address   = {Erscheinungsort},
    year      = {Erscheinungsjahr}
}

@article{Article,
    author    = {P.P. Autor},
    title     = {Titel des Artikels},
    journal   = {Titel der  Zeitschrift},
    volume    = {Band},
    pages     = {ersteSeite-letzteSeite},
    year      = {Jahr}
}

@misc{Webpage,
    author    = {P.P. Autor},
    title     = {Titel der www-Seite},
    howpublished = {http-Adresse},
    note      = {Zugriffsdatum}
}

@article{Chui_Approximation_1992,
    author    = {Charles K. Chui and Xin Li},
    title     = {Approximation by ridge functions and neural networks with one hidden layer},
    journal   = {Journal of Approximation Theory },
    volume    = {70},
    number    = {2},
    pages     = {131 - 141},
    year      = {1992},
    issn      = {0021-9045},
    doi       = {http://dx.doi.org/10.1016/0021-9045(92)90081-X},
    url       = {http://www.sciencedirect.com/science/article/pii/002190459290081X},
    abstract  = {We describe the configuration of an infinite set V of vectors in Rs, s ⩾ 1, for which the closure with respect to C(K) of the algebraic span of {ƒ(〈v, ·〉):v ϵ V, ƒ ϵ C(R)} is all of C(K), where K is any compact set in Rs. This configuration also guarantees that for any sigmoidal function σ ge C(R), the span of {σ(m〈v, · 〉 +k):v ϵ V;m, k ϵ Z} is already dense in C(K). In particular, neural networks with one hidden layer of the form ∑(I,k) ϵ J c(i,k) σ(〈i,x〉+k), where k ϵ Z, c(i, k) ϵ R, and i ϵ Zs, can be designed to approximate any continuous functions in s variables.}
}

@article{Chui_Realization_1992,
	author    = {C K Chui and X Li},
	title     = {Realization of neural networks with one hidden layer,” Center for Approximation Theory},
	journal   = {Dept. of Mathematics, Texas A\&M Univ., Tech. Rep},
	year      = {1991},
	number    = {244}
}

@article{DeVore_Optimal_1989,
	year      = {1989},
	issn      = {0025-2611},
	journal   = {manuscripta mathematica},
	volume    = {63},
	number    = {4},
	doi       = {10.1007/BF01171759},
	title     = {Optimal nonlinear approximation},
	url       = {http://dx.doi.org/10.1007/BF01171759},
	publisher = {Springer-Verlag},
	author    = {DeVore, RonaldA. and Howard, Ralph and Micchelli, Charles},
	pages     = {469-478},
	language  = {English}
}

@book{Hecht_Neurocomputing_1990,
	title     = {Neurocomputing},
	author    = {Hecht-Nielsen, R.},
	isbn      = {9780201093551},
	series    = {New Horizons in Technology Series},
	url       = {http://books.google.de/books?id=6YRQAAAAMAAJ},
	year      = {1990},
	publisher = {Addison-Wesley Publishing Company}
}

@article{Lenze_Note_1994,
    author    = {B. Lenze},
    title     = {Note on a density question for neural networks},
    journal   = {Numerical Funct. Analysis and Optimiz.},
    volume    = {15},
    pages     = {909--913},
    year      = {1994}
}

@book{Lenze_Einfuehrung_2000,
	title     = {Einführung in die Fourier-Analysis},
	author    = {B. Lenze},
	isbn      = {9783931216467},
    edition   = {zweite Auflage},
    address   = {Berlin},
	year      = {2000},
	publisher = {Logos Verlag}
}

@book{Lenze_Einfuehrung_2003,
	title     = {Einführung in die Mathematik neuronaler Netze},
	author    = {B. Lenze},
	isbn      = {9783897220218},
    edition   = {zweite Auflage},
    address   = {Berlin},
	year      = {2003},
	publisher = {Logos Verlag}
}

@book{Leshno_Multilayer_1993,
	title     = {Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function},
	author    = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
	journal   = {Neural Networks},
    volume    = {6},
    pages     = {861--867},
	year      = {1993},
	publisher = {Logos Verlag}
}

@article{Mhaskar_Approximation_1993,
	title     = {Approximation properties of a multilayered feedforward artificial neural network},
	author    = {Mhaskar, H.N.},
	journal   = {Advances in Computational Mathematics},
	volume    = {1},
	pages     = {61--80},
	year      = {1993},
	issn      = {1019-7168},
	number    = {1},
	doi       = {10.1007/BF02070821},
	url       = {http://dx.doi.org/10.1007/BF02070821},
	publisher = {Baltzer Science Publishers, Baarn/Kluwer Academic Publishers},
	keywords  = {Neural networks; uniform approximation; multivariate splines; analytic functions; modulus of smoothness; (AMS) 41A15; 41A63},
	language  = {English}
}

@article{Mhaskar_Neural_1996,
	title     = {Neural networks for optimal approximation of smooth and analytic functions},
	author    = {Mhaskar, H.N.},
	journal   = {Neural Computation},
	volume    = {8},
	pages     = {164--177},
	year      = {1996}
}

@article{Mhaskar_Approximation_1992,
	title     = {Approximation by superposition of sigmoidal and radial basis functions},
	journal   = {Advances in Applied Mathematics},
	volume    = {13},
	number    = {3},
	pages     = {350 -- 373},
	year      = {1992},
	issn      = {0196-8858},
	doi       = {http://dx.doi.org/10.1016/0196-8858(92)90016-P},
	url       = {http://www.sciencedirect.com/science/article/pii/019688589290016P},
	author    = {H.N Mhaskar and Charles A Micchelli},
	abstract  = {Let σ: R → R be such that for some polynomial P, σP is bounded. We consider the linear span of the functions {σ(λ · (x − t)): λ, t ϵ Rs}. We prove that unless σ is itself a polynomial, it is possible to uniformly approximate any continuous function on Rs arbitrarily well on every compact subset of Rs by functions in this span. Under more specific conditions on σ, we give algorithms to achieve this approximation and obtain Jackson-type theorems to estimate the degree of approximation.}
}

@article{Mhaskar_Degree_1995,
	title     = {Degree of Approximation by Neural and Translation Networks with a Single Hidden Layer},
	journal   = {Advances in Applied Mathematics},
	volume    = {16},
	number    = {2},
	pages     = {151 -- 183},
	year      = {1995},
	issn      = {0196-8858},
	doi       = {http://dx.doi.org/10.1006/aama.1995.1008},
	url       = {http://www.sciencedirect.com/science/article/pii/S0196885885710081},
	author    = {H.N Mhaskar and Charles A Micchelli},
	abstract  = {Let s ≥ d ≥ 1 be integers, 1 ≤ p &lt; ∞. We investigate the degree of approximation of 2π-periodic functions in Lp[−π, π]s (resp. C[− π, π]s) by finite linear combinations of translates and (matrix) dilates of a 2π-periodic function in Lp[−π, π]d (resp. C[− π, π]d). Applications to the theory of neural networks and radial basis approximation of functions which are not necessarily periodic are also discussed. In particular, we estimate the order of approximation by radial basis functions in terms of the number of translates involved in the approximating function.}
}

@book{Mueller_Neural_1995,
	title     = {Neural Networks},
	author    = {Müller, Berndt and Reinhardt, Joachim and Strickland, Michael T.},
	isbn      = {9783540602071},
    edition   = {2},
	series    = {Physics of Neural Networks},
	year      = {1995},
	publisher = {Springer-Verlag Berlin Heidelberg}
}

@article{Pinkus_TDI_1996,
	title     = {TDI-Subspaces of $C(\mathbf{R}^d)$ and Some Density Problems from Neural Networks},
	journal   = {Journal of Approximation Theory},
	volume    = {85},
	number    = {3},
	pages     = {269 -- 287},
	year      = {1996},
	issn      = {0021-9045},
	doi       = {http://dx.doi.org/10.1006/jath.1996.0042},
	url       = {http://www.sciencedirect.com/science/article/pii/	S0021904596900428},
	author    = {Allan Pinkus},
	abstract  = {In this paper we consider translation and dilation invariant subspaces of C(Rd). We characterize all such subspaces and also identify those f∈C(Rd), the span of whose translates and dilates generate non-trivial subspaces. We apply these and related results to some mathematical models in the theory of neural networks.}
}
